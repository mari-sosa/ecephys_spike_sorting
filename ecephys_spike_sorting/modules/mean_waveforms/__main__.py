from argschema import ArgSchemaParser
import os
import subprocess
import logging
import time

import numpy as np
import pandas as pd
from scipy.io import loadmat

from ...common.utils import load_kilosort_data

from .extract_waveforms import extract_waveforms, writeDataAsNpy
from .waveform_metrics import calculate_waveform_metrics
from .metrics_from_file import metrics_from_file

def calculate_mean_waveforms(args):

    print('ecephys spike sorting: mean waveforms module')
    
    start = time.time()
    
    if args['mean_waveform_params']['use_C_Waves']:
        
        print('Calculating mean waveforms using C_waves.')
        spikeglx_bin = args['ephys_params']['ap_band_file']
        # build paths to cluster and times tables, which are generated by
        # kilosort_helper module
        clus_table_npy = os.path.join(args['directories']['kilosort_output_directory'], 'clus_Table.npy' )
        clus_time_npy = os.path.join(args['directories']['kilosort_output_directory'], 'spike_times.npy' )
        clus_lbl_npy = os.path.join(args['directories']['kilosort_output_directory'], 'spike_clusters.npy' )
        dest, wavefile = os.path.split(args['mean_waveform_params']['mean_waveforms_file'])
        
        exe_path = os.path.join(args['mean_waveform_params']['cWaves_path'], 'runit.bat')
        
        cwaves_cmd = exe_path + ' -spikeglx_bin=' + spikeglx_bin + \
                                ' -clus_table_npy=' + clus_table_npy + \
                                ' -clus_time_npy=' + clus_time_npy + \
                                ' -clus_lbl_npy=' + clus_lbl_npy + \
                                ' -dest=' + dest + \
                                ' -samples_per_spike=' + repr(args['mean_waveform_params']['samples_per_spike']) + \
                                ' -pre_samples=' + repr(args['mean_waveform_params']['pre_samples']) + \
                                ' -num_spikes=' + repr(args['mean_waveform_params']['spikes_per_epoch']) + \
                                ' -snr_radius=' + repr(args['mean_waveform_params']['snr_radius'])
                                
        print(cwaves_cmd)
        
        # make the C_Waves call
        subprocess.call(cwaves_cmd)

        
        # C_Waves writes out files of the waveforms and snr
        # call version of calculate_waveform_metrics that will use these files
        # load in kilosort output needed for these calculations
        spike_times, spike_clusters, spike_templates, amplitudes, templates, channel_map, \
        channel_pos, clusterIDs, cluster_quality, cluster_amplitude = \
                load_kilosort_data(args['directories']['kilosort_output_directory'], \
                    args['ephys_params']['sample_rate'], \
                    convert_to_seconds = False)
                
        # read in inverse of whitening matrix
        w_inv = np.load((os.path.join(args['directories']['kilosort_output_directory'], 'whitening_mat_inv.npy')))
        
        # the channel_pos loaded from the phy output omits any sites excluded
        # as noise by the kilosort_helper module, or excluded fow low spike rete
        # by kilosort itself. The waveform metrics are calculated on ALL sites
        # based on the mean waveforms calculated for each unit; therefore
        # we need the site locations for all sites.
        # load the channel map associated with this kilosort run; in kilosort_helper
        # a copy is made next to the data file
        input_file = args['ephys_params']['ap_band_file']
        dat_dir, dat_name = os.path.split(input_file)
        chanMapMat = os.path.join(dat_dir,'chanMap.mat')
        site_x = np.squeeze(loadmat(chanMapMat)['xcoords'])
        site_y = np.squeeze(loadmat(chanMapMat)['ycoords'])
        
                
        mean_waveform_fullpath = os.path.join(dest, 'mean_waveforms.npy')
        snr_fullpath = os.path.join(dest, 'cluster_snr.npy')
                
        metrics = metrics_from_file(mean_waveform_fullpath, snr_fullpath, \
                    spike_times, \
                    spike_clusters, \
                    templates, \
                    channel_map, \
                    args['ephys_params']['bit_volts'], \
                    args['ephys_params']['sample_rate'], \
                    args['ephys_params']['vertical_site_spacing'], \
                    w_inv, \
                    site_x, site_y, \
                    args['mean_waveform_params'])
                
        metrics.to_csv(args['waveform_metrics']['waveform_metrics_file'])      
        
    else:
        
        print('Calculating mean waveforms using python.')
        print("Loading data...")
    
        rawData = np.memmap(args['ephys_params']['ap_band_file'], dtype='int16', mode='r')
        data = np.reshape(rawData, (int(rawData.size/args['ephys_params']['num_channels']), args['ephys_params']['num_channels']))
    
        spike_times, spike_clusters, spike_templates, amplitudes, templates, channel_map, \
        channel_pos, clusterIDs, cluster_quality, cluster_amplitude = \
                load_kilosort_data(args['directories']['kilosort_output_directory'], \
                    args['ephys_params']['sample_rate'], \
                    convert_to_seconds = False)
    
        print("Calculating mean waveforms...")
    
        waveforms, spike_counts, coords, labels, metrics = extract_waveforms(data, spike_times, \
                    spike_clusters,
                    templates,
                    channel_map,
                    args['ephys_params']['bit_volts'], \
                    args['ephys_params']['sample_rate'], \
                    args['ephys_params']['vertical_site_spacing'], \
                    args['mean_waveform_params'])
    
        writeDataAsNpy(waveforms, args['mean_waveform_params']['mean_waveforms_file'])
        metrics.to_csv(args['waveform_metrics']['waveform_metrics_file'])


    # if the cluster metrics have already been run, merge the waveform metrics into that file
    if os.path.exists(args['cluster_metrics']['cluster_metrics_file']):
        qmetrics = pd.read_csv(args['cluster_metrics']['cluster_metrics_file'])
        qmetrics = qmetrics.drop(qmetrics.columns[0], axis='columns')
        qmetrics = qmetrics.merge(pd.read_csv(args['waveform_metrics']['waveform_metrics_file'], index_col=0),
                     on='cluster_id',
                     suffixes=('_quality_metrics','_waveform_metrics'))  
        print("Saving merged quality metrics ...")
        qmetrics.to_csv(args['cluster_metrics']['cluster_metrics_file'])
        
    execution_time = time.time() - start

    print('total time: ' + str(np.around(execution_time,2)) + ' seconds')
    print()
    
    return {"execution_time" : execution_time} # output manifest


def main():

    from ._schemas import InputParameters, OutputParameters

    mod = ArgSchemaParser(schema_type=InputParameters,
                          output_schema_type=OutputParameters)

    output = calculate_mean_waveforms(mod.args)

    output.update({"input_parameters": mod.args})
    if "output_json" in mod.args:
        mod.output(output, indent=2)
    else:
        print(mod.get_output_json(output))


if __name__ == "__main__":
    main()

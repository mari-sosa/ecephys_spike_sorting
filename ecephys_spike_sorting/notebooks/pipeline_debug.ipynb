{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "focal-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from ecephys_spike_sorting.scripts.helpers import SpikeGLX_utils\n",
    "from ecephys_spike_sorting.scripts.helpers import log_from_json\n",
    "#from helpers import run_one_probe\n",
    "from ecephys_spike_sorting.scripts.create_input_json import createInputJson\n",
    "\n",
    "\n",
    "# script to run CatGT, KS2, postprocessing and TPrime on data collected using\n",
    "# SpikeGLX. The construction of the paths assumes data was saved with\n",
    "# \"Folder per probe\" selected (probes stored in separate folders) AND\n",
    "# that CatGT is run with the -out_prb_fld option\n",
    "\n",
    "# -------------------------------\n",
    "# -------------------------------\n",
    "# Start user input -- Edit this section\n",
    "# -------------------------------\n",
    "# -------------------------------\n",
    "run_file_base = \"20201224_C25R1_Day21_CenterRow180_TipRef\"\n",
    "\n",
    "# Raw data directory = npx_directory\n",
    "# This should be the parent directory for spikeglx data, \n",
    "# not the specific recording directory\n",
    "npx_directory = r'/opt/handeldata/rig43/DATA/'\n",
    "\n",
    "# brain region specific params\n",
    "# can add a new brain region by adding the key and value for each param\n",
    "# can add new parameters -- any that are taken by create_input_json --\n",
    "# by adding a new dictionary with entries for each region and setting the \n",
    "# according to the new dictionary in the loop to that created json files.\n",
    "# refPerMS is the refractory period threshold for the ISI distribution, per brain region.\n",
    "refPerMS_dict = {'default': 2.0, 'cortex': 2.0}\n",
    "\n",
    "# threhold values appropriate for KS2, KS2.5\n",
    "ksTh_dict = {'default':'[10,4]', 'cortex':'[9,4]'}\n",
    "# threshold values appropriate for KS3.0\n",
    "#ksTh_dict = {'default':'[9,9]', 'cortex':'[9,9]', 'medulla':'[9,9]', 'thalamus':'[9,9]'}\n",
    "\n",
    "\n",
    "# ------------------\n",
    "# Output destination\n",
    "# ------------------\n",
    "# Set to an existing directory; all output will be written here.\n",
    "# Output will be in the standard SpikeGLX directory structure:\n",
    "# run_folder/probe_folder/*.bin\n",
    "catGT_dest = os.path.join('/opt/handeldata/rig43/preprocessed/', run_file_base) #20201002_MS2_Day4_Bank2/'\n",
    "\n",
    "# -----------\n",
    "# Input data\n",
    "# -----------\n",
    "# Name for log file for this pipeline run. Log file will be saved in the\n",
    "# output destination directory catGT_dest\n",
    "# If this file exists, new run data is appended to it\n",
    "logName = f'{run_file_base}_log.csv'\n",
    "\n",
    "\n",
    "# run_specs = name, gate, trigger and probes to process\n",
    "# Each run_spec is a list of 5 strings:\n",
    "#   1. undecorated run name (no g/t specifier, the run field in CatGT until the gate, i.e. '_g0')\n",
    "#   2. gate index, as a string (e.g. '0', or 'start','last' e.g. '0,4')\n",
    "#   3. triggers to process/concatenate, as a string e.g. '0,400', '0,0 for a single file\n",
    "#           can replace first limit with 'start', last with 'end'; 'start,end'\n",
    "#           will concatenate all trials in the probe folder\n",
    "#   4. probes to process, as a string, e.g. '0', '0,3', '0:3'\n",
    "#   5. brain regions, list of strings, one per probe, to set region specific params\n",
    "#           these strings must match a key in the param dictionaries above.\n",
    "\n",
    "run_specs = [\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t[run_file_base, '3,4', '0,0', '0',['cortex'] ]\n",
    "            ]\n",
    "#run_specs = [\t\t\t\t\t\t\t\t\t\n",
    "#\t    \t['SC024_092319_NP1.0_Midbrain', '0', '0,9', '0,1', ['cortex', 'medulla'] ]\n",
    "#]\n",
    "\n",
    "# ------------\n",
    "# CatGT params\n",
    "# ------------\n",
    "run_CatGT = False   # set to False to sort/process previously processed data.\n",
    "\n",
    "\n",
    "# CAR mode for CatGT. Must be equal to 'None', 'gbldmx', or 'loccar'\n",
    "car_mode = 'gbldmx'\n",
    "# inner and outer radii, in um for local comman average reference, if used\n",
    "loccar_min = 40\n",
    "loccar_max = 160\n",
    "\n",
    "# CatGT commands for bandpass filtering, artifact correction, and zero filling\n",
    "# Note 1: directory naming in this script requires -prb_fld and -out_prb_fld\n",
    "# Note 2: this command line includes specification of edge extraction\n",
    "# see CatGT readme for details\n",
    "# these parameters will be used for all runs\n",
    "\n",
    "# gfix=0,0.10,0.02 -- artifact removal; params: |thresh_amp(mV)|,|slope(mV/sample)|,noise\n",
    "# -t_miss_ok option required to concatenate over missing g or t indices\n",
    "# -zerofillmax=500 option required to fill gaps only up to 500ms of zeros,\n",
    "# so kilsort doesn't crash\n",
    "catGT_cmd_string = '-t_miss_ok -zerofillmax=500 -prb_fld -out_prb_fld -aphipass=300 -aplopass=6000 -lflopass=400 -gfix=0,0.10,0.02'\n",
    "catGT_stream_string = '-ap -ni -lf'\n",
    "\n",
    "ni_present = True\n",
    "# ni_extract_string = '-XA=0,1,3,500 -XA=1,3,3,0 -XD=4,1,50 -XD=4,2,1.7 -XD=4,3,5'\n",
    "\n",
    "# ----- NIDAQ INPUTS -----\n",
    "# -- Each XA gets its own word, starting with 0. XA inputs must come first.\n",
    "# -- XD inputs come next, on separate words from the XA inputs. \n",
    "# -- Each XD word contains up to 16 bits (0:15)\n",
    "# -- rig43 inputs: --\n",
    "# XA=0,1,3,500 -- sync channel on nidaq: word 0, thresh 1 V, must stay above 3V, dur 500 ms\n",
    "# XA=1,1,1.5,0 -- camera: word 1, thresh 1 V, must stay above 1.5V, dur 10  ms\n",
    "#                - check baseline and pulse height per animal/session\n",
    "#                - duration must be within +/-20% of the actual pulse width; 0 ignores pulse width requirement\n",
    "# XD=2,0,0 -- Well 0 LED: word 0, bit 0, dur 0  ms (for MS2 day4, these are IR beam break)\n",
    "# XD=2,1,0 -- Well 1 LED: word 0, bit 1, dur 0  ms\n",
    "# XD=2,2,0 -- Well 2 LED: word 0, bit 2, dur 0  ms\n",
    "# XD=2,3,0 -- Well 3 LED: word 0, bit 3, dur 0  ms\n",
    "# XD=2,4,0 -- Well 0 IR detect: word 0, bit 4, dur 0  ms\n",
    "# XD=2,5,0 -- Well 1 IR detect: word 0, bit 5, dur 0  ms\n",
    "# XD=2,6,0 -- Well 2 IR detect: word 0, bit 6, dur 0  ms\n",
    "# XD=2,7,0 -- Well 3 IR detect: word 0, bit 7, dur 0  ms\n",
    "# XD=2,8,0 -- Well 0 IR beam break: word 0, bit 8, dur 0  ms (for MS2 day4, theses are LED)\n",
    "# XD=2,9,0 -- Well 1  IR beam break: word 0, bit 9, dur 0  ms\n",
    "# XD=2,10,0 -- Well 2 IR beam break: word 0, bit 10, dur 0  ms\n",
    "# XD=2,11,0 -- Well 3 IR beam break: word 0, bit 11, dur 0  ms\n",
    "# XD=2,12,0 -- Well 0 pump: word 0, bit 12, dur 0  ms\n",
    "# XD=2,13,0 -- Well 1 pump: word 0, bit 13, dur 0  ms\n",
    "# XD=2,14,0 -- Well 2 pump: word 0, bit 14, dur 0  ms\n",
    "# XD=2,15,0 -- Well 3 pump: word 0, bit 15, dur 0  ms\n",
    "\n",
    "ni_extract_string = '-XA=0,1,3,500 '\\\n",
    "        '-XA=1,2.5,2.49,0 '\\\n",
    "        '-XD=2,0,0 '\\\n",
    "        '-XD=2,1,0 '\\\n",
    "        '-XD=2,2,0 '\\\n",
    "        '-XD=2,3,0 '\\\n",
    "        '-iXD=2,0,0 '\\\n",
    "        '-iXD=2,1,0 '\\\n",
    "        '-iXD=2,2,0 '\\\n",
    "        '-iXD=2,3,0 '\\\n",
    "        '-XD=2,4,0 '\\\n",
    "        '-XD=2,5,0 '\\\n",
    "        '-XD=2,6,0 '\\\n",
    "        '-XD=2,7,0 '\\\n",
    "        '-iXD=2,8,0 '\\\n",
    "        '-iXD=2,9,0 '\\\n",
    "        '-iXD=2,10,0 '\\\n",
    "        '-iXD=2,11,0 '\\\n",
    "        '-XD=2,12,0 '\\\n",
    "        '-XD=2,13,0 '\\\n",
    "        '-XD=2,14,0 '\\\n",
    "        '-XD=2,15,0'\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# KS2 or KS25 parameters\n",
    "# ----------------------\n",
    "# parameters that will be constant for all recordings\n",
    "# Template ekmplate radius and whitening, which are specified in um, will be \n",
    "# translated into sites using the probe geometry.\n",
    "ks_remDup = 0\n",
    "ks_saveRez = 1\n",
    "ks_copy_fproc = 0\n",
    "ks_templateRadius_um = 163\n",
    "ks_whiteningRadius_um = 163\n",
    "ks_minfr_goodchannels = 0.05 #0.1\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# C_Waves snr radius, um\n",
    "# ----------------------\n",
    "c_Waves_snr_um = 160\n",
    "\n",
    "# ----------------------\n",
    "# psth_events parameters\n",
    "# ----------------------\n",
    "# extract param string for psth events -- copy the CatGT params used to extract\n",
    "# events that should be exported with the phy output for PSTH plots\n",
    "# If not using, remove psth_events from the list of modules\n",
    "event_ex_param_str = 'XD=4,1,50'\n",
    "\n",
    "# -----------------\n",
    "# TPrime parameters\n",
    "# -----------------\n",
    "runTPrime = True   # set to False if not using TPrime\n",
    "sync_period = 1.0   # true for SYNC wave generated by imec basestation\n",
    "toStream_sync_params = 'SY=0,-1,6,500'  # copy from the CatGT command line, no spaces\n",
    "niStream_sync_params = 'XA=0,1,3,500'   # copy from the CatGT comman line, set to None if no Aux data, no spaces\n",
    "\n",
    "# ---------------\n",
    "# Modules List\n",
    "# ---------------\n",
    "# List of modules to run per probe; CatGT and TPrime are called once for each run.\n",
    "# M.S. removed 'psth_events'\n",
    "modules = [\n",
    "            'kilosort_helper',\n",
    "            'kilosort_postprocessing',\n",
    "            'noise_templates',\n",
    "            'mean_waveforms',\n",
    "            'quality_metrics'\n",
    "\t\t\t]\n",
    "\n",
    "json_directory = os.path.join('/opt/handeldata/rig43/preprocessed', run_file_base) #20201002_MS2_Day4_Bank2' \n",
    "\n",
    "# -----------------------\n",
    "# -----------------------\n",
    "# End of user input\n",
    "# -----------------------\n",
    "# -----------------------\n",
    "\n",
    "# delete the existing CatGT.log\n",
    "try:\n",
    "    os.remove('CatGT.log')\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "# delete existing Tprime.log\n",
    "try:\n",
    "    os.remove('Tprime.log')\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "# delete existing C_waves.log\n",
    "try:\n",
    "    os.remove('C_Waves.log')\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "# check for existence of log file, create if not there\n",
    "logFullPath = os.path.join(catGT_dest, logName)\n",
    "if not os.path.isfile(logFullPath):\n",
    "    # create the log file, write header\n",
    "    log_from_json.writeHeader(logFullPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imposed-isaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3,4\n"
     ]
    }
   ],
   "source": [
    "for spec in run_specs:\n",
    "    print(spec[1])\n",
    "spec=run_specs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "trained-vietnam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2-3]\n",
      "2-3\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "glist = ''\n",
    "g_range = '[' + spec[1][0] + '-' + spec[1][-1] + ']'\n",
    "first_gate = spec[1][0]\n",
    "print(g_range)\n",
    "# get list of g-indices to concatenate from data directory\n",
    "g_tocat = glob.glob(os.path.join(npx_directory,(run_file_base + '_g' + g_range)))\n",
    "glist = ''.join((x[-1]+'-') for x in g_tocat)[:-1] # g inds separated by dashes, minus the last dash\n",
    "\n",
    "# for i, name in enumerate(g_tocat): \n",
    "#     if i==len(g_tocat)-1:\n",
    "#         glist += name[-1]\n",
    "#     else:\n",
    "#         glist += (name[-1] + '-')\n",
    "\n",
    "print(glist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "drawn-priority",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/handeldata/rig43/preprocessed/20201224_C25R1_Day21_CenterRow180_TipRef'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catGT_dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-frame",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "operating-month",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "renaming catgt output...\n",
      "renamed 26 files or directries in catgt output dir.\n"
     ]
    }
   ],
   "source": [
    "# debugging make believe stuff\n",
    "glist = '3-4'\n",
    "first_gate = '3'\n",
    "catgt_output_dir = os.path.join(catGT_dest,'firstrun','catgt_20201224_C25R1_Day21_CenterRow180_TipRef_g3_20210303')\n",
    "\n",
    "\n",
    "if len(glist)>len(first_gate):\n",
    "    f_to_rename = glob.glob((catgt_output_dir + '/**/*_g' + first_gate + '_*'),recursive=True)\n",
    "    print('renaming catgt output...')\n",
    "    for f in f_to_rename:         \n",
    "        splt_f = f.rsplit(('_g' + first_gate), 1)\n",
    "        new_f = ('_g' + glist).join(splt_f)\n",
    "        #new_f = f.replace(('_g' + first_gate),('_g' + glist))\n",
    "        if os.path.isdir(f):\n",
    "            mv_cmd = \"mv \" + f + \" \" + new_f\n",
    "            subprocess.call(mv_cmd,shell=True)\n",
    "            subf_to_rename =  glob.glob((new_f + '/**/*_g' + first_gate + '_*'),recursive=True)\n",
    "            for sf in subf_to_rename:\n",
    "                splt_f = sf.rsplit(('_g' + first_gate), 1)\n",
    "                new_f = ('_g' + glist).join(splt_f)\n",
    "                os.rename(sf,new_f)\n",
    "        else:\n",
    "            if os.path.isfile(f):\n",
    "                os.rename(f,new_f)\n",
    "    print(f\"renamed {len(f_to_rename)} files or directries in catgt output dir.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "small-pitch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating g indices 2-3\n",
      "Creating json file for CatGT on probe: 0\n",
      "first gate 2\n",
      "gate string 2,3\n",
      "SpikeGLX params read from meta\n",
      "probe type: NP1, sample_rate: 30000.31229, num_channels: 385, uVPerBit: 2.3438\n",
      "kilosort output directory: /opt/handeldata/rig43/preprocessed/20201224_C25R1_Day21_CenterRow180_TipRef\n",
      "renaming catgt output...\n",
      "['/opt/handeldata/rig43/preprocessed/20201224_C25R1_Day21_CenterRow180_TipRef/catgt_20201224_C25R1_Day21_CenterRow180_TipRef_g2-3/20201224_C25R1_Day21_CenterRow180_TipRef_g2-3_imec0/20201224_C25R1_Day21_CenterRow180_TipRef_g2_imec0']\n",
      "/opt/handeldata/rig43/preprocessed/20201224_C25R1_Day21_CenterRow180_TipRef/catgt_20201224_C25R1_Day21_CenterRow180_TipRef_g2-3/20201224_C25R1_Day21_CenterRow180_TipRef_g2-3_imec0/20201224_C25R1_Day21_CenterRow180_TipRef_g2-3_imec0\n",
      "ks_Th: '[9,4]' ,refPerMS: 2.0\n",
      "SpikeGLX params read from meta\n",
      "probe type: NP1, sample_rate: 30000.31229, num_channels: 385, uVPerBit: 2.3438\n",
      "kilosort output directory: /opt/handeldata/rig43/preprocessed/20201224_C25R1_Day21_CenterRow180_TipRef/catgt_20201224_C25R1_Day21_CenterRow180_TipRef_g2-3/20201224_C25R1_Day21_CenterRow180_TipRef_g2-3_imec0/imec0_ks2\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['python', '-W', 'ignore', '-m', 'ecephys_spike_sorting.modules.kilosort_helper', '--input_json', '/opt/handeldata/rig43/preprocessed/20201224_C25R1_Day21_CenterRow180_TipRef/20201224_C25R1_Day21_CenterRow180_TipRef_g2-3_imec0-input.json', '--output_json', '/opt/handeldata/rig43/preprocessed/20201224_C25R1_Day21_CenterRow180_TipRef/20201224_C25R1_Day21_CenterRow180_TipRef_g2-3_imec0-kilosort_helper-output.json']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-602b3c8013c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"python -W ignore -m ecephys_spike_sorting.modules.\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" --input_json \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_input_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                           \u001b[0;34m+\u001b[0m \u001b[0;34m\" --output_json \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_output_json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# copy json file to data directory as record of the input parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcmd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopenargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['python', '-W', 'ignore', '-m', 'ecephys_spike_sorting.modules.kilosort_helper', '--input_json', '/opt/handeldata/rig43/preprocessed/20201224_C25R1_Day21_CenterRow180_TipRef/20201224_C25R1_Day21_CenterRow180_TipRef_g2-3_imec0-input.json', '--output_json', '/opt/handeldata/rig43/preprocessed/20201224_C25R1_Day21_CenterRow180_TipRef/20201224_C25R1_Day21_CenterRow180_TipRef_g2-3_imec0-kilosort_helper-output.json']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "for spec in run_specs:\n",
    "\n",
    "    session_id = spec[0]\n",
    "\n",
    "    \n",
    "    # Make list of probes from the probe string\n",
    "    prb_list = SpikeGLX_utils.ParseProbeStr(spec[3])\n",
    "    \n",
    "    # build path to the first probe folder; look into that folder\n",
    "    # to determine the range of trials if the user specified t limits as\n",
    "    # start and end\n",
    "    run_folder_name = spec[0] + '_g' + spec[1]\n",
    "    prb0_fld_name = run_folder_name + '_imec' + prb_list[0]\n",
    "    prb0_fld = os.path.join(npx_directory, run_folder_name, prb0_fld_name)\n",
    "    first_trig, last_trig = SpikeGLX_utils.ParseTrigStr(spec[2], prb_list[0], spec[1], prb0_fld)\n",
    "    trigger_str = repr(first_trig) + ',' + repr(last_trig)\n",
    "    \n",
    "    # get list of g-indices to concatenate from data directory\n",
    "    first_gate = spec[1][0]\n",
    "    g_range = '[' + spec[1][0] + '-' + spec[1][-1] + ']'\n",
    "    g_tocat = glob.glob(os.path.join(npx_directory,(run_file_base + '_g' + g_range)))\n",
    "    glist = ''.join((x[-1]+'-') for x in g_tocat)[:-1] # g inds separated by dashes, minus the last dash\n",
    "\n",
    "    print('Concatenating g indices ' + glist)\n",
    "    \n",
    "    # loop over all probes to build json files of input parameters\n",
    "    # initalize lists for input and output json files\n",
    "    catGT_input_json = []\n",
    "    catGT_output_json = []\n",
    "    module_input_json = []\n",
    "    module_output_json = []\n",
    "    session_id = []\n",
    "    catgt_output_dir = []\n",
    "    data_directory = []\n",
    "    \n",
    "    # first loop over probes creates json files containing parameters for\n",
    "    # both preprocessing (CatGt) and sorting + postprocessing\n",
    "    \n",
    "    for i, prb in enumerate(prb_list):\n",
    "            \n",
    "        #create CatGT command for this probe\n",
    "        print('Creating json file for CatGT on probe: ' + prb)\n",
    "        catGT_input_json.append(os.path.join(json_directory, spec[0] + '_g' + glist + '_prb' + prb + '_CatGT' + '-input.json'))\n",
    "        catGT_output_json.append(os.path.join(json_directory, spec[0] + '_g' + glist + '_prb' + prb + '_CatGT' + '-output.json'))\n",
    "        \n",
    "        # build extract string for SYNC channel for this probe\n",
    "        sync_extract = '-SY=' + prb +',-1,6,500'\n",
    "        \n",
    "        # if this is the first probe proceessed, process the ni stream with it\n",
    "        if i == 0 and ni_present:\n",
    "            catGT_stream_string = '-ap -ni -lf'\n",
    "            extract_string = sync_extract + ' ' + ni_extract_string\n",
    "        else:\n",
    "            catGT_stream_string = '-ap -lf'\n",
    "            extract_string = sync_extract\n",
    "        \n",
    "        # build name of first trial/gate to be concatenated/processed;\n",
    "        # allows reading of the metadata\n",
    "        print('first gate ' + spec[1][0])\n",
    "        print('gate string ' + spec[1])\n",
    "        run_str = spec[0] + '_g' + spec[1][0] \n",
    "        run_folder = run_str\n",
    "        prb_folder = run_str + '_imec' + prb\n",
    "        input_data_directory = os.path.join(npx_directory, run_folder, prb_folder)\n",
    "        fileName = run_str + '_t' + repr(first_trig) + '.imec' + prb + '.ap.bin'\n",
    "        continuous_file = os.path.join(input_data_directory, fileName)\n",
    "        metaName = run_str + '_t' + repr(first_trig) + '.imec' + prb + '.ap.meta'\n",
    "        input_meta_fullpath = os.path.join(input_data_directory, metaName)\n",
    "        \n",
    "        # ----- RUN CatGT -----\n",
    "        info = createInputJson(catGT_input_json[i], npx_directory=npx_directory, \n",
    "                                       continuous_file = continuous_file,\n",
    "                                       kilosort_output_directory=catGT_dest,\n",
    "                                       spikeGLX_data = True,\n",
    "                                       input_meta_path = input_meta_fullpath,\n",
    "                                       catGT_run_name = spec[0],\n",
    "                                       gate_string = spec[1],\n",
    "                                       gate_list_string = glist,\n",
    "                                       trigger_string = trigger_str,\n",
    "                                       probe_string = prb,\n",
    "                                       catGT_stream_string = catGT_stream_string,\n",
    "                                       catGT_car_mode = car_mode,\n",
    "                                       catGT_loccar_min_um = loccar_min,\n",
    "                                       catGT_loccar_max_um = loccar_max,\n",
    "                                       catGT_cmd_string = catGT_cmd_string + ' ' + extract_string,\n",
    "                                       extracted_data_directory = catGT_dest\n",
    "                                       )      \n",
    "        \n",
    "        \n",
    "        if run_CatGT:\n",
    "            command = \"python -W ignore -m ecephys_spike_sorting.modules.\" + 'catGT_helper' + \" --input_json \" + catGT_input_json[i] \\\n",
    "            \t          + \" --output_json \" + catGT_output_json[i]\n",
    "            subprocess.check_call(command.split(' '))           \n",
    "\n",
    "            # parse the CatGT log and write results to command line\n",
    "            print(f\"probe_list {prb_list}\")\n",
    "            logPath = os.getcwd()\n",
    "            gfix_edits = SpikeGLX_utils.ParseCatGTLog( logPath, spec[0], spec[1], prb_list )\n",
    "        \n",
    "            for i in range(0,len(prb_list)):\n",
    "                edit_string = '{:.3f}'.format(gfix_edits[i])\n",
    "                print('Probe ' + prb_list[i] + '; gfix edits/sec: ' + repr(gfix_edits[i]))\n",
    "        else:\n",
    "            # fill in dummy gfix_edits for running without preprocessing\n",
    "            gfix_edits = np.zeros(len(prb_list), dtype='float64' )\n",
    "        \n",
    "        #create json files for the other modules\n",
    "        session_id.append(spec[0] + '_g' + glist + '_imec' + prb)\n",
    "        \n",
    "        module_input_json.append(os.path.join(json_directory, session_id[i] + '-input.json'))\n",
    "        \n",
    "        \n",
    "        # location of the binary created by CatGT, using -out_prb_fld\n",
    "        # use glist for run string here: included g indices separated by dashes \n",
    "        run_str = spec[0] + '_g' + glist\n",
    "        run_folder = 'catgt_' + run_str\n",
    "        prb_folder = run_str + '_imec' + prb\n",
    "        catgt_output_dir = os.path.join(catGT_dest, run_folder)\n",
    "        data_directory.append(os.path.join(catGT_dest, run_folder, prb_folder))\n",
    "        fileName = run_str + '_tcat.imec' + prb + '.ap.bin'\n",
    "        continuous_file = os.path.join(data_directory[i], fileName)\n",
    " \n",
    "        outputName = 'imec' + prb + '_ks2'\n",
    "    \n",
    "        # recursively rename files in the catgt output dir to match the gate list,\n",
    "        #      if more than 1 gate was concatenated\n",
    "        # first get files in the renamed directory matching the first g index\n",
    "        if len(glist)>len(first_gate):\n",
    "            f_to_rename = glob.glob((catgt_output_dir + '/**/*_g' + first_gate + '_*'),recursive=True)\n",
    "            print('renaming catgt output...')\n",
    "            for f in f_to_rename: \n",
    "                splt_f = f.rsplit(('_g' + first_gate), 1)\n",
    "                new_f = ('_g' + glist).join(splt_f)\n",
    "                #new_f = f.replace(('_g' + first_gate),('_g' + glist))\n",
    "                if os.path.isdir(f):\n",
    "                    mv_cmd = \"mv \" + f + \" \" + new_f\n",
    "                    subprocess.call(mv_cmd,shell=True)\n",
    "                    subf_to_rename =  glob.glob((new_f + '/**/*_g' + first_gate + '_*'),recursive=True)\n",
    "                    print(subf_to_rename)\n",
    "                    for sf in subf_to_rename:\n",
    "                        splt_f = sf.rsplit(('_g' + first_gate), 1)\n",
    "                        new_f = ('_g' + glist).join(splt_f)\n",
    "                        os.rename(sf,new_f)\n",
    "                else:\n",
    "                    os.rename(f,new_f)\n",
    "                print(new_f)\n",
    "\n",
    "        # kilosort_postprocessing and noise_templates moduules alter the files\n",
    "        # that are input to phy. If using these modules, keep a copy of the\n",
    "        # original phy output\n",
    "        if ('kilosort_postprocessing' in modules) or('noise_templates' in modules):\n",
    "            ks_make_copy = True\n",
    "        else:\n",
    "            ks_make_copy = False\n",
    "\n",
    "        kilosort_output_dir = os.path.join(data_directory[i], outputName)\n",
    "\n",
    "        # get region specific parameters\n",
    "        ks_Th = ksTh_dict.get(spec[4][i])\n",
    "        refPerMS = refPerMS_dict.get(spec[4][i])\n",
    "        print( 'ks_Th: ' + repr(ks_Th) + ' ,refPerMS: ' + repr(refPerMS))\n",
    "\n",
    "        info = createInputJson(module_input_json[i], npx_directory=npx_directory, \n",
    "\t                               continuous_file = continuous_file,\n",
    "                                       spikeGLX_data = True,\n",
    "                                       input_meta_path = input_meta_fullpath,\n",
    "\t\t\t\t       kilosort_output_directory=kilosort_output_dir,\n",
    "                                       ks_make_copy = ks_make_copy,\n",
    "                                       noise_template_use_rf = False,\n",
    "                                       catGT_run_name = session_id[i],\n",
    "                                       gate_string = spec[1],\n",
    "                                       gate_list_string = glist,\n",
    "                                       probe_string = spec[3],  \n",
    "                                       ks_remDup = ks_remDup,                   \n",
    "                                       ks_finalSplits = 1,\n",
    "                                       ks_labelGood = 1,\n",
    "                                       ks_saveRez = ks_saveRez,\n",
    "                                       ks_copy_fproc = ks_copy_fproc,\n",
    "                                       ks_minfr_goodchannels = ks_minfr_goodchannels,                  \n",
    "                                       ks_whiteningRadius_um = ks_whiteningRadius_um,\n",
    "                                       ks_Th = ks_Th,\n",
    "                                       ks_CSBseed = 1,\n",
    "                                       ks_LTseed = 1,\n",
    "                                       ks_templateRadius_um = ks_templateRadius_um,\n",
    "                                       extracted_data_directory = catGT_dest,\n",
    "                                       event_ex_param_str = event_ex_param_str,\n",
    "                                       c_Waves_snr_um = c_Waves_snr_um,                               \n",
    "                                       qm_isi_thresh = refPerMS/1000\n",
    "                                       )   \n",
    "\n",
    "        # Run each module --- KS is run here ---\n",
    "        for module in modules:\n",
    "            module_output_json = os.path.join(json_directory, session_id[i] + '-' + module + '-output.json')  \n",
    "            command = \"python -W ignore -m ecephys_spike_sorting.modules.\" + module + \" --input_json \" + module_input_json[i] \\\n",
    "\t\t          + \" --output_json \" + module_output_json\n",
    "            subprocess.check_call(command.split(' '))\n",
    "        \n",
    "        # copy json file to data directory as record of the input parameters \n",
    "        log_from_json.addEntry(modules, json_directory, session_id[i], logFullPath)\n",
    "        \n",
    "    # loop over probes for processing.    \n",
    "   # for i, prb in enumerate(prb_list):  \n",
    "   #     \n",
    "   #     run_one_probe.runOne( session_id[i],\n",
    "   #              json_directory,\n",
    "   #              data_directory[i],\n",
    "   #              run_CatGT,\n",
    "   #              catGT_input_json[i],\n",
    "   #              catGT_output_json[i],\n",
    "   #              modules,\n",
    "   #              module_input_json[i],\n",
    "   #              logFullPath )\n",
    "                 \n",
    "    # ----- RUN TPrime -----\n",
    "\n",
    "    if runTPrime:\n",
    "        # after loop over probes, run TPrime to create files of \n",
    "        # event times -- edges detected in auxialliary files and spike times \n",
    "        # from each probe -- all aligned to a reference stream.\n",
    "    \n",
    "        # create json files for calling TPrime\n",
    "        session_id = spec[0] + '_TPrime'\n",
    "        input_json = os.path.join(json_directory, session_id + '-input.json')\n",
    "        output_json = os.path.join(json_directory, session_id + '-output.json')\n",
    "        \n",
    "        # build list of sync extractions to send to TPrime\n",
    "        im_ex_list = ''\n",
    "        for i, prb in enumerate(prb_list):\n",
    "            sync_extract = '-SY=' + prb +',-1,6,500'\n",
    "            im_ex_list = im_ex_list + ' ' + sync_extract\n",
    "            \n",
    "        print('im_ex_list: ' + im_ex_list)     \n",
    "        \n",
    "        info = createInputJson(input_json, npx_directory=npx_directory, \n",
    "    \t                                   continuous_file = continuous_file,\n",
    "                                           spikeGLX_data = True,\n",
    "                                           input_meta_path = input_meta_fullpath,\n",
    "                                           catGT_run_name = spec[0],\n",
    "    \t\t\t\t\t\t\t\t\t   kilosort_output_directory=kilosort_output_dir,\n",
    "                                           extracted_data_directory = catGT_dest,\n",
    "                                           tPrime_im_ex_list = im_ex_list,\n",
    "                                           tPrime_ni_ex_list = ni_extract_string,\n",
    "                                           event_ex_param_str = event_ex_param_str,\n",
    "                                           sync_period = 1.0,\n",
    "                                           toStream_sync_params = toStream_sync_params,\n",
    "                                           niStream_sync_params = niStream_sync_params,\n",
    "                                           tPrime_3A = False,\n",
    "                                           toStream_path_3A = ' ',\n",
    "                                           fromStream_list_3A = list()\n",
    "                                           ) \n",
    "        \n",
    "        command = \"python -W ignore -m ecephys_spike_sorting.modules.\" + 'tPrime_helper' + \" --input_json \" + input_json \\\n",
    "    \t\t          + \" --output_json \" + output_json\n",
    "        subprocess.check_call(command.split(' '))  \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "failing-event",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_CatGT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "disabled-report",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "renamed 1 files or directries in catgt output dir.\n"
     ]
    }
   ],
   "source": [
    "len(f_to_rename)\n",
    "print(f\"renamed {len(f_to_rename)} files or directries in catgt output dir.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "touched-copying",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'XD'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exstr = \"XD=2,0,0\"\n",
    "exstr.split('=')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alleged-garbage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20201224_C25R1_Day21_CenterRow180_TipRef']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "electronic-compiler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/handeldata/rig43/preprocessed/20201224_C25R1_Day21_CenterRow180_TipRef/catgt_20201224_C25R1_Day21_CenterRow180_TipRef_g0/20201224_C25R1_Day21_CenterRow180_TipRef_g0_imec0/20201224_C25R1_Day21_CenterRow180_TipRef_g0_tcat.imec0.ap.bin'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continuous_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "legislative-jurisdiction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr(first_trig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "banned-viewer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rig43/local_repos/ecephys_spike_sorting/ecephys_spike_sorting/notebooks'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "premium-binding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.call(\"mv /home/rig43/local_repos/ecephys_spike_sorting/ecephys_spike_sorting/notebooks/testdir /home/rig43/local_repos/ecephys_spike_sorting/ecephys_spike_sorting/notebooks/new_testdir\",shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "classical-ticket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testdir/test_g2-3.txt\n",
      "testdir/test_g2-3-3.txt\n",
      "testdir/test3_g2-3.txt\n",
      "testdir/test3_g2-3-3.txt\n",
      "testdir/test2_g2-3.txt\n",
      "testdir/test2_g2-3-3.txt\n",
      "testdir/subdir_g2-3\n",
      "testdir/subdir_g2-3-3\n",
      "testdir/subdir_g2-3/subtest_g2.txt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'testdir/subdir_g2-3/subtest_g2.txt' -> 'testdir/subdir_g2-3-3/subtest_g2-3.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-7b4d11cd0cd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnew_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_g'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfirst_gate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_g'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mglist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'testdir/subdir_g2-3/subtest_g2.txt' -> 'testdir/subdir_g2-3-3/subtest_g2-3.txt'"
     ]
    }
   ],
   "source": [
    "# get files in the renamed directory matching the first g index\n",
    "f_to_rename = glob.glob(('testdir/**/*_g' + first_gate + '*'),recursive=True)\n",
    "\n",
    "for f in f_to_rename:\n",
    "    print(f)\n",
    "    new_f = f.replace(('_g' + first_gate),('_g' + glist))\n",
    "    os.rename(f,new_f)\n",
    "    print(new_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "great-citizenship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rig43/local_repos/ecephys_spike_sorting/ecephys_spike_sorting/notebooks'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "black-chuck",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readdir = 'testdir'\n",
    "writedir = 'newtestdir'\n",
    "mv_cmd = 'mv ' + readdir + ' ' + writedir\n",
    "subprocess.call(mv_cmd,shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "green-world",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mv testdir newtestdir'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mv_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "sought-chuck",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test2dir'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = 'testdir'.rsplit('t', 1)\n",
    "'t2'.join(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "indian-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "str.rsplit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-mortgage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
